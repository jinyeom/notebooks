{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Estimation from Videos\n",
    "**Jin Yeom**  \n",
    "jin.yeom@hudl.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import ArtistAnimation\n",
    "from IPython.display import HTML\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sys.path.append('/openpose/build/python')\n",
    "    from openpose import pyopenpose as op\n",
    "except ImportError as e:\n",
    "    print(\"Error: OpenPose library could not be found. Did you enable `BUILD_PYTHON` in CMake and have this Python script in the right folder?\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    model_folder='/openpose/models/',\n",
    "    number_people_max = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\n",
    "    [255, 0, 0], \n",
    "    [255, 85, 0], \n",
    "    [255, 170, 0], \n",
    "    [255, 255, 0], \n",
    "    [170, 255, 0], \n",
    "    [85, 255, 0], \n",
    "    [0, 255, 0], \n",
    "    [0, 255, 85], \n",
    "    [0, 255, 170], \n",
    "    [0, 255, 255], \n",
    "    [0, 170, 255], \n",
    "    [0, 85, 255], \n",
    "    [0, 0, 255],\n",
    "    [85, 0, 255], \n",
    "    [170, 0, 255],\n",
    "    [255, 0, 255],\n",
    "    [255, 85, 255]\n",
    "]\n",
    "\n",
    "# NOTE: this corresponds to COCO dataset\n",
    "joint_pairs = [\n",
    "    [0, 1], \n",
    "    [1, 3], \n",
    "    [0, 2], \n",
    "    [2, 4],\n",
    "    [5, 6], \n",
    "    [5, 7], \n",
    "    [7, 9], \n",
    "    [6, 8], \n",
    "    [8, 10],\n",
    "    [5, 11], \n",
    "    [6, 12], \n",
    "    [11, 12],\n",
    "    [11, 13], \n",
    "    [12, 14], \n",
    "    [13, 15], \n",
    "    [14, 16]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert openpose keypoints (25) format to COCO keypoints (17) format\n",
    "def op2coco(op_kpts):\n",
    "    coco_kpts = []\n",
    "    for i, j in enumerate([0, 16, 15, 18, 17, 5, 2, 6, 3, 7, 4, 12, 9, 13, 10, 14, 11]):\n",
    "        score = op_kpts[j][-1]\n",
    "        # if eye and ear keypoints score is lower, map it to mouth\n",
    "        if score < 0.2 and j in [15, 16, 17, 18]:\n",
    "            coco_kpts.append(op_kpts[0])\n",
    "        else:\n",
    "            coco_kpts.append(op_kpts[j])\n",
    "    return np.stack(coco_kpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_pose(image, keypoints, keypoint_thresh=0.1):\n",
    "    confidence = keypoints[:, 2:]\n",
    "    coordinates = keypoints[:, :2]\n",
    "    joint_visible = confidence[:, 0] > keypoint_thresh\n",
    "    \n",
    "    for color, (x, y, c) in zip(colors, keypoints):\n",
    "        if c < keypoint_thresh:\n",
    "            continue\n",
    "        cv2.circle(image, (int(x), int(y)), 4, color, thickness=-1)\n",
    "    \n",
    "    for color, jp in zip(colors, joint_pairs):\n",
    "        if joint_visible[jp[0]] and joint_visible[jp[1]]:\n",
    "            pt0 = coordinates[jp, 0]\n",
    "            pt1 = coordinates[jp, 1]\n",
    "            pt0_0, pt0_1, pt1_0, pt1_1 = int(pt0[0]), int(pt0[1]), int(pt1[0]), int(pt1[1])\n",
    "            cv2.line(image, (pt0_0, pt1_0), (pt0_1, pt1_1), color, 2)\n",
    "            \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting OpenPose\n",
    "op_wrapper = op.WrapperPython()\n",
    "op_wrapper.configure(params)\n",
    "op_wrapper.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video2frames(filename, start_time, end_time, sx=0.5, sy=0.5, frame_skip=4):\n",
    "    video = cv2.VideoCapture(filename)\n",
    "\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    start_frame = int(fps * start_time)  \n",
    "    end_frame = int(fps * end_time)\n",
    "    n_frames = end_frame - start_frame\n",
    "\n",
    "    for i in tqdm(range(start_frame, end_frame, frame_skip)):\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        success, frame = video.read()\n",
    "        height = int(sy * frame.shape[0])\n",
    "        width = int(sx * frame.shape[1])\n",
    "        frame = cv2.resize(frame, (width, height))  # make it smaller\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # to RGB\n",
    "        yield frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbfe24168c5e4f9f8afda615f5635aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1349), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Requested MovieWriter (ffmpeg) not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavail\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ffmpeg'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e2a0d8f58e50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0manime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArtistAnimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_html5_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36mto_html5_video\u001b[0;34m(self, embed_limit)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                 \u001b[0;31m# We create a writer manually so that we can get the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                 \u001b[0;31m# appropriate size for the tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m                 \u001b[0mWriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwriters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'animation.writer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m                 writer = Writer(codec='h264',\n\u001b[1;32m   1328\u001b[0m                                 \u001b[0mbitrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'animation.bitrate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise RuntimeError(\n\u001b[0;32m--> 164\u001b[0;31m                 'Requested MovieWriter ({}) not available'.format(name))\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Requested MovieWriter (ffmpeg) not available"
     ]
    }
   ],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 8), dpi=120)\n",
    "ax0.set_axis_off()\n",
    "ax1.set_axis_off()\n",
    "\n",
    "ims = []\n",
    "for i, frame in enumerate(video2frames('data/gd.mp4', 0, 225)):\n",
    "    datum = op.Datum()\n",
    "    datum.cvInputData = frame\n",
    "    op_wrapper.emplaceAndPop([datum])\n",
    "    keypoints = datum.poseKeypoints\n",
    "    \n",
    "    im0 = ax0.imshow(frame)\n",
    "    if len(keypoints.shape):\n",
    "        keypoints = op2coco(keypoints[0])\n",
    "        im1 = ax1.imshow(with_pose(frame, keypoints))\n",
    "    else:\n",
    "        im1 = ax1.imshow(frame)\n",
    "    ims.append([im0, im1])\n",
    "    \n",
    "anime = ArtistAnimation(fig, ims, interval=30, blit=True)\n",
    "plt.close(fig=fig)\n",
    "HTML(anime.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/CMU-Perceptual-Computing-Lab/openpose\n",
    "- https://github.com/esemeniuc/openpose-docker\n",
    "- https://github.com/lxy5513/videopose"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
