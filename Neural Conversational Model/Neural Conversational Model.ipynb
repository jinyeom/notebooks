{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Conversational Model\n",
    "**Jin Yeom**  \n",
    "jin.yeom@hudl.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reproduces [this tutorial](https://pytorch.org/tutorials/beginner/chatbot_tutorial.html) from the official documentation page of PyTorch. While the tutorial itself can be quite interesting, our focus will be learning how to work with sequence data and recurrent neural networks. Hopefully, we'll never have to do anything with natural language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch version:', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by downloading the [Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p datasets\n",
    "cd datasets\n",
    "wget -q http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
    "unzip -q cornell_movie_dialogs_corpus.zip\n",
    "rm cornell_movie_dialogs_corpus.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'datasets/cornell movie-dialogs corpus'\n",
    "lines_path = os.path.join(dataset_path, 'movie_lines.txt')\n",
    "convs_path = os.path.join(dataset_path, 'movie_conversations.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek(filename, n=10):\n",
    "    with open(filename, 'rb') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:n]:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n'\n",
      "b'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\\n'\n",
      "b'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\\n'\n",
      "b'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\\n'\n",
      "b\"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\\n\"\n",
      "b'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\\n'\n",
      "b\"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\\n\"\n",
      "b'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\\n'\n",
      "b'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?\\n'\n",
      "b'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\\n'\n"
     ]
    }
   ],
   "source": [
    "peek(lines_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with preprocessing the data to the correct format!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lines(filename, fields):\n",
    "    lines = {}\n",
    "    with open(filename, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            values = line.split(' +++$+++ ')\n",
    "            line_obj = {}\n",
    "            for i, field in enumerate(fields):\n",
    "                line_obj[field] = values[i]\n",
    "            lines[line_obj['lineID']] = line_obj\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_convs(filename, lines, fields):\n",
    "    convs = []\n",
    "    with open(filename, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            values = line.split(' +++$+++ ')\n",
    "            conv_obj = {}\n",
    "            for i,field in enumerate(fields):\n",
    "                conv_obj[field] = values[i]\n",
    "            line_ids = eval(conv_obj['utteranceIDs'])\n",
    "            conv_obj['lines'] = []\n",
    "            for line_id in line_ids:\n",
    "                conv_obj['lines'].append(lines[line_id])\n",
    "            convs.append(conv_obj)\n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentence_pairs(convs):\n",
    "    qa_pairs = []\n",
    "    for conv in convs:\n",
    "        for i in range(len(conv['lines']) - 1):\n",
    "            input_line = conv['lines'][i]['text'].strip()\n",
    "            target_line = conv['lines'][i+1]['text'].strip()\n",
    "            if input_line and target_line:\n",
    "                qa_pairs.append((input_line, target_line))\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corpus...done\n",
      "Loading conversations...done\n",
      "Writing formatted file...done\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(dataset_path, 'formatted_movie_lines.txt')\n",
    "delimiter = str(codecs.decode('\\t', 'unicode_escape'))\n",
    "\n",
    "lines = {}\n",
    "convs = {}\n",
    "lines_fields = ['lineID', 'characterID', 'movieID', 'character', 'text']\n",
    "convs_fields = ['character1ID', 'character2ID', 'movieID', 'utteranceIDs']\n",
    "\n",
    "print(\"Processing corpus...\", end='')\n",
    "lines = load_lines(lines_path, lines_fields)\n",
    "print(\"done\")\n",
    "\n",
    "print(\"Loading conversations...\", end='')\n",
    "convs = load_convs(convs_path, lines, convs_fields)\n",
    "print(\"done\")\n",
    "\n",
    "print(\"Writing formatted file...\", end='')\n",
    "with open(data_path, 'w', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, delimiter=delimiter, lineterminator='\\n')\n",
    "    for pair in extract_sentence_pairs(convs):\n",
    "        writer.writerow(pair)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\"\n",
      "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\n\"\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"\n",
      "b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\n\"\n",
      "b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\n\"\n",
      "b\"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\n\"\n",
      "b\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\\n\"\n",
      "b'Why?\\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\n'\n",
      "b\"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\tThat's a shame.\\n\"\n",
      "b'Gosh, if only we could find Kat a boyfriend...\\tLet me see what I can do.\\n'\n"
     ]
    }
   ],
   "source": [
    "peek(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's process the formatted data further to a suitable format for training. Here, we create some vocabulary features, which include mapping from words to their indices, reverse mapping from indices to words, a count of each word and total word count. Additionally, we'll trim down words whose counts are below a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.words = ['PAD', 'SOS', 'EOS']\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.count = 3\n",
    "        \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "            \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.words.append(word)\n",
    "            self.word2index[word] = self.count\n",
    "            self.word2count[word] = 1\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return        \n",
    "        trimmed = []\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                trimmed.append(k)\n",
    "        print(f\"Trimmed from {self.count} words to {len(trimmed)} words\")\n",
    "        print(f\"Down to {len(trimmed)/self.count*100}%\")\n",
    "        self.reset()\n",
    "        for word in trimmed:\n",
    "            self.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode2ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(s):\n",
    "    s = unicode2ascii(s.lower().strip())\n",
    "    s = re.sub(r'([.!?])', r' \\1', s)\n",
    "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
    "    return re.sub(r'\\s+', r' ', s).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vocs(filename, corpus):\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        lines = f.read().strip().split('\\n')\n",
    "    pairs = [[normalize(s) for s in l.split('\\t')] for l in lines]\n",
    "    voc = Voc(corpus)\n",
    "    return voc, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pair(p, max_length):\n",
    "    return (len(p[0].split(' ')) < max_length and\n",
    "            len(p[1].split(' ')) < max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pairs(pairs, max_length):\n",
    "    return [p for p in pairs if filter_pair(p, max_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, corpus, max_length=10):\n",
    "    voc, pairs = read_vocs(filename, corpus)\n",
    "    print(f\"Read {len(pairs)} sentence pairs\")\n",
    "    pairs = filter_pairs(pairs, max_length)\n",
    "    print(f\"Filtered to {len(pairs)} sentence pairs\")\n",
    "    for p in pairs:\n",
    "        voc.add_sentence(p[0])\n",
    "        voc.add_sentence(p[1])\n",
    "    print(f\"Word count: {voc.count}\")\n",
    "    return voc, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 221282 sentence pairs\n",
      "Filtered to 64271 sentence pairs\n",
      "Word count: 18008\n"
     ]
    }
   ],
   "source": [
    "voc, pairs = load_data(data_path, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there .', 'where ?']\n",
      "['you have my word . as a gentleman', 'you re sweet .']\n",
      "['hi .', 'looks like things worked out tonight huh ?']\n",
      "['you know chastity ?', 'i believe we share an art instructor']\n",
      "['have fun tonight ?', 'tons']\n",
      "['well no . . .', 'then that s all you had to say .']\n",
      "['then that s all you had to say .', 'but']\n",
      "['but', 'you always been this selfish ?']\n",
      "['do you listen to this crap ?', 'what crap ?']\n",
      "['what good stuff ?', 'the real you .']\n"
     ]
    }
   ],
   "source": [
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we trim down words that are less frequently used. More specifically, we'll trim words that appear less than or equal to 3 times total in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_words(voc, pairs, min_count=3):\n",
    "    voc.trim(min_count)\n",
    "    trimmed = []\n",
    "    for p in pairs:\n",
    "        valid_input = all(w in voc.word2index for w in p[0].split(' '))\n",
    "        valid_output = all(w in voc.word2index for w in p[1].split(' '))\n",
    "        if valid_input and valid_output:\n",
    "            trimmed.append(p)\n",
    "    print(f\"Trimmed from {len(pairs)} pairs to {len(trimmed)} pairs\")\n",
    "    print(f\"Down to {len(trimmed)/len(pairs)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed from 18008 words to 7823 words\n",
      "Down to 43.44180364282541%\n",
      "Trimmed from 64271 pairs to 53165 pairs\n",
      "Down to 82.72004481025657%\n"
     ]
    }
   ],
   "source": [
    "pairs = trim_words(voc, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I guess this makes sense: 82% of our data uses 43% of the vocabulary more than 3 times, other 67% of the words are rather unique that they don't come up as often. Ideally, we'd like our agent to be able to quickly adopt those rare words, but we'll just pretend they don't exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation for training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks",
   "language": "python",
   "name": "notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
