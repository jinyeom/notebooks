{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Conversational Model\n",
    "**Jin Yeom**  \n",
    "jin.yeom@hudl.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reproduces [this tutorial](https://pytorch.org/tutorials/beginner/chatbot_tutorial.html) from the official documentation page of PyTorch. While the tutorial itself is quite interesting, our focus will be learning how to work with sequence data and recurrent neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "print('PyTorch version:', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by downloading the [Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir datasets\n",
    "cd datasets\n",
    "wget -q http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
    "unzip -q cornell_movie_dialogs_corpus.zip\n",
    "rm cornell_movie_dialogs_corpus.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'datasets/cornell movie-dialogs corpus'\n",
    "lines_path = os.path.join(dataset_path, 'movie_lines.txt')\n",
    "convs_path = os.path.join(dataset_path, 'movie_conversations.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek(filename, n=10):\n",
    "    with open(filename, 'rb') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:n]:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n'\n",
      "b'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\\n'\n",
      "b'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\\n'\n",
      "b'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\\n'\n",
      "b\"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\\n\"\n",
      "b'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\\n'\n",
      "b\"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\\n\"\n",
      "b'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\\n'\n",
      "b'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?\\n'\n",
      "b'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\\n'\n"
     ]
    }
   ],
   "source": [
    "peek(lines_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for some data preprocessing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lines(filename, fields):\n",
    "    lines = {}\n",
    "    with open(filename, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            values = line.split(' +++$+++ ')\n",
    "            line_obj = {}\n",
    "            for i, field in enumerate(fields):\n",
    "                line_obj[field] = values[i]\n",
    "            lines[line_obj['lineID']] = line_obj\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_convs(filename, lines, fields):\n",
    "    convs = []\n",
    "    with open(filename, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            values = line.split(' +++$+++ ')\n",
    "            conv_obj = {}\n",
    "            for i,field in enumerate(fields):\n",
    "                conv_obj[field] = values[i]\n",
    "            line_ids = eval(conv_obj['utteranceIDs'])\n",
    "            conv_obj['lines'] = []\n",
    "            for line_id in line_ids:\n",
    "                conv_obj['lines'].append(lines[line_id])\n",
    "            convs.append(conv_obj)\n",
    "    return convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentence_pairs(convs):\n",
    "    qa_pairs = []\n",
    "    for conv in convs:\n",
    "        for i in range(len(conv['lines']) - 1):\n",
    "            input_line = conv['lines'][i]['text'].strip()\n",
    "            target_line = conv['lines'][i+1]['text'].strip()\n",
    "            if input_line and target_line:\n",
    "                qa_pairs.append((input_line, target_line))\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corpus...done\n",
      "Loading conversations...done\n",
      "Writing formatted file...done\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(dataset_path, 'formatted_movie_lines.txt')\n",
    "delimiter = str(codecs.decode('\\t', 'unicode_escape'))\n",
    "\n",
    "lines = {}\n",
    "convs = {}\n",
    "lines_fields = ['lineID', 'characterID', 'movieID', 'character', 'text']\n",
    "convs_fields = ['character1ID', 'character2ID', 'movieID', 'utteranceIDs']\n",
    "\n",
    "print(\"Processing corpus...\", end='')\n",
    "lines = load_lines(lines_path, lines_fields)\n",
    "print(\"done\")\n",
    "\n",
    "print(\"Loading conversations...\", end='')\n",
    "convs = load_convs(convs_path, lines, convs_fields)\n",
    "print(\"done\")\n",
    "\n",
    "print(\"Writing formatted file...\", end='')\n",
    "with open(data_path, 'w', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, delimiter=delimiter, lineterminator='\\n')\n",
    "    for pair in extract_sentence_pairs(convs):\n",
    "        writer.writerow(pair)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\"\n",
      "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\n\"\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"\n",
      "b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\n\"\n",
      "b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\n\"\n",
      "b\"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\n\"\n",
      "b\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\\n\"\n",
      "b'Why?\\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\n'\n",
      "b\"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\tThat's a shame.\\n\"\n",
      "b'Gosh, if only we could find Kat a boyfriend...\\tLet me see what I can do.\\n'\n"
     ]
    }
   ],
   "source": [
    "peek(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
